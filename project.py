import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv('crx.csv', header=None, na_values='?')

# Lo·∫°i b·ªè header d√≤ng ƒë·∫ßu (n·∫øu c√≥)
if df.iloc[0].astype(str).str.contains('A2').any():
    df = df.iloc[1:].reset_index(drop=True)

# G√°n t√™n c·ªôt
df.columns = ['A'+str(i) for i in range(1, 17)]

# X·ª≠ l√Ω missing v√† ki·ªÉu d·ªØ li·ªáu
for col in df.columns:
    if df[col].dtype == 'object':
        try:
            df[col] = df[col].astype('float64')
        except:
            df[col] = df[col].fillna(df[col].mode()[0])
    if df[col].dtype == 'float64':
        df[col] = df[col].fillna(df[col].mean())

# M√£ h√≥a nh√£n
df['label'] = LabelEncoder().fit_transform(df['A16'])

# M√£ h√≥a nh·ªã ph√¢n
le = LabelEncoder()
for col in ['A1', 'A9', 'A10', 'A12']:
    df[col] = le.fit_transform(df[col])

# One-hot encoding c√°c c·ªôt ph√¢n lo·∫°i
df = pd.get_dummies(df, columns=['A4','A5','A6','A7','A13'], drop_first=False)

# T√°ch ƒë·∫∑c tr∆∞ng v√† nh√£n
feature_cols = [c for c in df.columns if c not in ['A16', 'label']]
X_raw = df[feature_cols].copy()
y = df['label'].values

# M√¥ h√¨nh c·∫ßn ƒë√°nh gi√°
models = {
    'Naive Bayes': GaussianNB(),
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Decision Tree': DecisionTreeClassifier(criterion='entropy', random_state=0)
}

results = {name: [] for name in models}
avg_results = {}

# ƒê√°nh gi√° 10 l·∫ßn
for i in range(10):
    X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=i)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, y_pred)
        results[name].append(acc)

# In k·∫øt qu·∫£
print("üéØ K·∫øt qu·∫£ t·ª´ng l·∫ßn v√† ƒë·ªô ch√≠nh x√°c trung b√¨nh:\n")
for name, scores in results.items():
    avg = np.mean(scores)
    print(f"{name} (avg: {avg:.4f})")
    for i, acc in enumerate(scores, 1):
        print(f"  L·∫ßn {i:2d}: {acc:.4f}")
    avg_results[name] = avg
    print()

# Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t
best_model_name = max(avg_results, key=avg_results.get)
print(f"üèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} v·ªõi accuracy trung b√¨nh: {avg_results[best_model_name]:.4f}")
final_model = models[best_model_name]

# Hu·∫•n luy·ªán l·∫°i tr√™n to√†n b·ªô t·∫≠p
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_raw)
final_model.fit(X_scaled, y)

# L∆∞u feature names ƒë·ªÉ Flask predict kh·ªõp
feature_names = X_raw.columns.tolist()

# L∆∞u m√¥ h√¨nh + scaler + t√™n c·ªôt
joblib.dump({
    'model': final_model,
    'scaler': scaler,
    'feature_names': feature_names
}, 'best_model.pkl')

print("üíæ ƒê√£ l∆∞u m√¥ h√¨nh v√†o 'best_model.pkl'")

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8,5))
plt.bar(avg_results.keys(), avg_results.values(), color='teal')
plt.title('So s√°nh ƒë·ªô ch√≠nh x√°c trung b√¨nh c·ªßa c√°c m√¥ h√¨nh (10 l·∫ßn ch·∫°y)')
plt.xlabel('M√¥ h√¨nh')
plt.ylabel('Accuracy trung b√¨nh')
plt.ylim(0, 1)
for i, v in enumerate(avg_results.values()):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center')
plt.tight_layout()
plt.show()
