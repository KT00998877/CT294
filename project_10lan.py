import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# ƒê·ªçc d·ªØ li·ªáu
original_df = pd.read_csv('crx.csv')  
df = original_df.copy()    

# X·ª≠ l√Ω d·ªØ li·ªáu b·ªã thi·∫øu
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

# Encode nh√£n A16
le_y = LabelEncoder()
le_y.fit(['-', '+'])  # √©p '+' l√† 0, '-' l√† 1
df['A16'] = le_y.transform(df['A16'])
label_mapping = dict(zip(le_y.classes_, le_y.transform(le_y.classes_)))
print("üìå √Ånh x·∫° nh√£n A16:", label_mapping)

# One-Hot Encoding c√°c c·ªôt object (tr·ª´ A16)
df = pd.get_dummies(df, columns=df.select_dtypes(include=['object']).columns, drop_first=True)

# T√°ch X v√† y
X = df.drop(columns=['A16'])
y = df['A16']

# Kh·ªüi t·∫°o m√¥ h√¨nh
models = {
    'KNN': KNeighborsClassifier(n_neighbors=5),
    'Decision Tree': DecisionTreeClassifier(max_depth=5),
    'Naive Bayes': GaussianNB()
}

results = {name: [] for name in models}  # L∆∞u k·∫øt qu·∫£ c·ªßa t·ª´ng l·∫ßn ch·∫°y

# L·∫∑p l·∫°i 10 l·∫ßn
for run in range(1, 11):
    # Chia d·ªØ li·ªáu train/test ng·∫´u nhi√™n m·ªói l·∫ßn
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state= run + 42
    )

    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # ƒê√°nh gi√° t·ª´ng m√¥ h√¨nh
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        acc = accuracy_score(y_test, y_pred)
        results[name].append(acc)

# Hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ng l·∫ßn v√† trung b√¨nh
print("K·∫øt qu·∫£ t·ª´ng l·∫ßn v√† trung b√¨nh:\n")
for name, acc_list in results.items():
    print(f"{name}:")
    for i, acc in enumerate(acc_list, 1):
        print(f"   L·∫ßn {i:2d}: {acc:.4f}")
    print(f"   Trung b√¨nh: {np.mean(acc_list):.4f}\n")

# T·∫°o DataFrame k·∫øt qu·∫£ trung b√¨nh
avg_results = {name: np.mean(acc_list) for name, acc_list in results.items()}
df_results = pd.DataFrame(
    [(k, v) for k, v in avg_results.items()],
    columns=['Model', 'Average Accuracy']
)

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(8, 5))
plt.bar(df_results['Model'], df_results['Average Accuracy'], color=['blue'])
plt.xlabel('Model')
plt.ylabel('Average Accuracy')
plt.title(' ƒê·ªô ch√≠nh x√°c trung b√¨nh c·ªßa c√°c m√¥ h√¨nh ')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

best_model_name = max(avg_results, key=avg_results.get)
best_acc = avg_results[best_model_name]

print(f"M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} v·ªõi ƒë·ªô ch√≠nh x√°c trung b√¨nh {best_acc:.4f}")

# Train m√¥ h√¨nh t·ªët nh·∫•t l·∫°i tr√™n to√†n b·ªô d·ªØ li·ªáu
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
best_model = models[best_model_name]
best_model.fit(X_scaled, y)

# L∆∞u m√¥ h√¨nh t·ªët nh·∫•t
joblib.dump({
    'model': best_model,
    'scaler': scaler,
    'feature_names': X.columns.tolist(), 
    'label_encoder_y': le_y
}, 'best_model.pkl')

import seaborn as sns
import matplotlib.pyplot as plt

# Gi·∫£ s·ª≠ b·∫°n ƒë√£ LabelEncode A16 nh∆∞ sau: '+' ‚Üí 1 (ƒê∆∞·ª£c duy·ªát), '-' ‚Üí 0 (Kh√¥ng duy·ªát)
sns.set(style="whitegrid")
plt.figure(figsize=(6, 4))
sns.countplot(x='A16', data=df, palette='pastel')

# ƒê·∫∑t nh√£n tr·ª•c x cho d·ªÖ hi·ªÉu
plt.xticks([1, 0], ['‚ùå Kh√¥ng duy·ªát', '‚úÖ ƒê∆∞·ª£c duy·ªát'])
plt.title('S·ªë l∆∞·ª£ng m·∫´u ƒë∆∞·ª£c duy·ªát v√† kh√¥ng ƒë∆∞·ª£c duy·ªát')
plt.xlabel('K·∫øt qu·∫£ ph√™ duy·ªát')
plt.ylabel('S·ªë l∆∞·ª£ng')
plt.tight_layout()
plt.show()

print("ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t v√†o file best_model.pkl")

# ƒê√°nh gi√° b·∫±ng ma tr·∫≠n nh·∫ßm l·∫´n
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# L·∫∑p qua t·ª´ng m√¥ h√¨nh
for name, model in models.items():
    # Chia t·∫≠p train/test
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Hu·∫•n luy·ªán v√† d·ª± ƒëo√°n
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)

    # Ma tr·∫≠n nh·∫ßm l·∫´n
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le_y.classes_)

    # V·∫Ω ri√™ng t·ª´ng ma tr·∫≠n
    plt.figure(figsize=(5, 4))
    disp.plot(cmap='Blues', values_format='d')
    plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {name}')
    plt.grid(False)
    plt.tight_layout()
    plt.show()


# ƒê√°nh gi√° b·∫±ng c√°c ch·ªâ s·ªë Precision / Recall / F1-score:

from sklearn.metrics import classification_report
import seaborn as sns

def plot_classification_report(model, model_name):
    # D·ª± ƒëo√°n tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu
    y_pred = model.predict(X_scaled)
    
    # T·∫°o classification report d·∫°ng dict
    report_dict = classification_report(y, y_pred, target_names=['‚ùå Kh√¥ng duy·ªát', '‚úÖ ƒê∆∞·ª£c duy·ªát'], output_dict=True)
    
    # B·ªè h√†ng 'accuracy', 'macro avg', 'weighted avg' n·∫øu kh√¥ng mu·ªën
    df_report = pd.DataFrame(report_dict).transpose().round(2)
    
    # V·∫Ω b·∫£ng ƒë·∫πp b·∫±ng seaborn
    plt.figure(figsize=(8, 3))
    sns.heatmap(df_report.iloc[:2, :3], annot=True, cmap='YlGnBu', cbar=False, fmt='.2f')
    plt.title(f'Classification Report - {model_name}')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

# G·ªçi h√†m cho m√¥ h√¨nh t·ªët nh·∫•t
plot_classification_report(best_model, best_model_name)

